{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klarh/geometric_algebra_attention/blob/master/examples/Molecular%20force%20regression%20using%20multivectors%20in%20pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Colab-specific setup that will be ignored elsewhere\n",
    "if [ ! -z \"$COLAB_GPU\" ]; then\n",
    "    pip install flowws-keras-geometry keras-gtar\n",
    "    pip install --force-reinstall git+https://github.com/klarh/flowws-keras-experimental\n",
    "    pip install git+https://github.com/klarh/geometric_algebra_attention\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch as pt\n",
    "print(pt.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowws\n",
    "from flowws_keras_geometry.data import RMD17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import geometric_algebra_attention.pytorch as gala\n",
    "\n",
    "class MomentumNorm(pt.nn.Module):\n",
    "    def __init__(self, n_dim, momentum=.99):\n",
    "        super().__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.register_buffer('momentum', pt.as_tensor(momentum))\n",
    "        self.register_buffer('mu', pt.zeros(n_dim))\n",
    "        self.register_buffer('sigma', pt.ones(n_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        axes = tuple(range(x.ndim - 1))\n",
    "        mu_calc = pt.mean(x, axes, keepdim=False)\n",
    "        sigma_calc = pt.std(x, axes, keepdim=False)\n",
    "\n",
    "        new_mu = self.momentum*self.mu + (1 - self.momentum)*mu_calc\n",
    "        new_sigma = self.momentum*self.sigma + (1 - self.momentum)*sigma_calc\n",
    "\n",
    "        if self.training:\n",
    "            self.mu[:] = new_mu.detach()\n",
    "            self.sigma[:] = new_sigma.detach()\n",
    "\n",
    "        sigma = pt.maximum(self.sigma, pt.as_tensor(1e-7))\n",
    "\n",
    "        return (x - self.mu.detach())/sigma.detach()\n",
    "\n",
    "class LayerNorm(pt.nn.Module):\n",
    "    def forward(self, x):\n",
    "        mu = pt.mean(x, -1, keepdim=True)\n",
    "        sigmasq = pt.var(x, -1, keepdim=True)\n",
    "        sigma = pt.sqrt(pt.maximum(sigmasq, pt.as_tensor(1e-7)))\n",
    "\n",
    "        return (x - mu.detach())/sigma.detach()\n",
    "\n",
    "class GalaPotential(pt.nn.Module):\n",
    "    \"\"\"Calculate a potential using geometric algebra attention\n",
    "\n",
    "    Stacks permutation-covariant attention blocks, then adds a permutation-invariant reduction layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, D=32, depth=2, dilation=2., residual=True,\n",
    "                 nonlinearities=True, merge_fun='mean', join_fun='mean',\n",
    "                 invariant_mode='single', rank=2,\n",
    "                 invar_value_normalization=None, value_normalization=None,\n",
    "                 score_normalization=None, block_normalization=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.D_in = D_in\n",
    "        self.D = D\n",
    "        self.depth = depth\n",
    "        self.dilation = dilation\n",
    "        self.residual = residual\n",
    "        self.nonlinearities = nonlinearities\n",
    "        self.rank = rank\n",
    "        self.invariant_mode = invariant_mode\n",
    "        self.GAANet_kwargs = dict(merge_fun=merge_fun, join_fun=join_fun, invariant_mode=invariant_mode)\n",
    "\n",
    "        self.invar_value_normalization = invar_value_normalization\n",
    "        self.value_normalization = value_normalization\n",
    "        self.score_normalization = score_normalization\n",
    "        self.block_normalization = block_normalization\n",
    "\n",
    "        self.vec2mv = gala.Vector2Multivector()\n",
    "        self.up_project = pt.nn.Linear(2*D_in, self.D)\n",
    "        self.final_mlp = self.make_value_net(self.D)\n",
    "        self.energy_projection = pt.nn.Linear(self.D, 1, bias=False)\n",
    "\n",
    "        self.make_attention_nets()\n",
    "\n",
    "        self.nonlin_mlps = []\n",
    "        if self.nonlinearities:\n",
    "            self.nonlin_mlps = pt.nn.ModuleList([\n",
    "                self.make_value_net(self.D, within_network=False) for _ in range(self.depth + 1)])\n",
    "\n",
    "        self.block_norm_layers = pt.nn.ModuleList([])\n",
    "        for _ in range(self.depth + 1):\n",
    "            self.block_norm_layers.extend(self._get_normalization_layers(self.block_normalization, self.D))\n",
    "\n",
    "    def make_attention_nets(self):\n",
    "        D_in = lambda i: 1 if (i == self.depth and self.rank == 1) else 2\n",
    "        self.score_nets = pt.nn.ModuleList([])\n",
    "        self.value_nets = pt.nn.ModuleList([])\n",
    "        self.scale_nets = pt.nn.ModuleList([])\n",
    "        self.eqvar_att_nets = pt.nn.ModuleList([])\n",
    "        self.invar_att_nets = pt.nn.ModuleList([])\n",
    "\n",
    "        for i in range(self.depth + 1):\n",
    "            reduce = i == self.depth\n",
    "            rank = max(2, self.rank) if not reduce else self.rank\n",
    "\n",
    "            # rotation-equivariant (multivector-producing) networks\n",
    "            self.score_nets.append(self.make_score_net())\n",
    "            self.value_nets.append(self.make_value_net(gala.Multivector2MultivectorAttention.get_invariant_dims(\n",
    "                self.rank, self.invariant_mode)))\n",
    "            self.scale_nets.append(self.make_score_net())\n",
    "            self.eqvar_att_nets.append(gala.Multivector2MultivectorAttention(\n",
    "                self.D, self.score_nets[-1], self.value_nets[-1], self.scale_nets[-1],\n",
    "                reduce=False, rank=rank, **self.GAANet_kwargs))\n",
    "\n",
    "            # rotation-invariant (node value-producing) networks\n",
    "            self.score_nets.append(self.make_score_net())\n",
    "            self.value_nets.append(self.make_value_net(gala.MultivectorAttention.get_invariant_dims(\n",
    "                self.rank, self.invariant_mode)))\n",
    "            self.invar_att_nets.append(gala.MultivectorAttention(\n",
    "                self.D, self.score_nets[-1], self.value_nets[-1],\n",
    "                reduce=reduce, rank=rank, **self.GAANet_kwargs))\n",
    "\n",
    "    def _get_normalization_layers(self, norm, n_dim):\n",
    "        if not norm:\n",
    "            return []\n",
    "        elif norm == 'momentum':\n",
    "            return [MomentumNorm(n_dim)]\n",
    "        elif norm == 'layer':\n",
    "            return [LayerNorm()]\n",
    "        else:\n",
    "            raise NotImplementedError(norm)\n",
    "\n",
    "    def make_score_net(self):\n",
    "        big_D = int(self.D*self.dilation)\n",
    "        layers = [\n",
    "            pt.nn.Linear(self.D, big_D),\n",
    "        ]\n",
    "\n",
    "        layers.extend(self._get_normalization_layers(self.score_normalization, big_D))\n",
    "\n",
    "        layers.extend([\n",
    "            pt.nn.SiLU(),\n",
    "            pt.nn.Linear(big_D, 1),\n",
    "        ])\n",
    "        return pt.nn.Sequential(*layers)\n",
    "\n",
    "    def make_value_net(self, D_in, D_out=None, within_network=True):\n",
    "        D_out = D_out or self.D\n",
    "        big_D = int(self.D*self.dilation)\n",
    "        layers = []\n",
    "\n",
    "        if within_network:\n",
    "            layers.extend(self._get_normalization_layers(self.invar_value_normalization, D_in))\n",
    "\n",
    "        layers.append(pt.nn.Linear(D_in, big_D))\n",
    "\n",
    "        layers.extend(self._get_normalization_layers(self.value_normalization, big_D))\n",
    "\n",
    "        layers.extend([\n",
    "            pt.nn.SiLU(),\n",
    "            pt.nn.Linear(big_D, D_out),\n",
    "        ])\n",
    "        return pt.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (r, v) = x\n",
    "        r = pt.as_tensor(r)\n",
    "        v = pt.as_tensor(v)\n",
    "\n",
    "        neighbor_rij = r[..., None, :, :] - r[..., :, None, :]\n",
    "        neighbor_rij = self.vec2mv(neighbor_rij)\n",
    "        vplus = v[..., None, :, :] + v[..., :, None, :]\n",
    "        vminus = v[..., None, :, :] - v[..., :, None, :]\n",
    "        neighbor_vij = pt.cat([vplus, vminus], axis=-1)\n",
    "\n",
    "        last_r = neighbor_rij\n",
    "        last = self.up_project(neighbor_vij)\n",
    "\n",
    "        for i in range(self.depth + 1):\n",
    "            residual = last\n",
    "            residual_r = last_r\n",
    "\n",
    "            last_r = self.eqvar_att_nets[i]((last_r, last))\n",
    "            last = self.invar_att_nets[i]((last_r, last))\n",
    "            if self.nonlinearities:\n",
    "                last = self.nonlin_mlps[i](last)\n",
    "\n",
    "            if self.residual and i < self.depth:\n",
    "                last = last + residual\n",
    "\n",
    "            if self.block_norm_layers:\n",
    "                last = self.block_norm_layers[i](last)\n",
    "\n",
    "            if self.residual:\n",
    "                last_r = last_r + residual_r\n",
    "            last_r = last_r + neighbor_rij\n",
    "\n",
    "        last = self.final_mlp(last)\n",
    "        last = pt.sum(last, -2)\n",
    "        last = self.energy_projection(last)\n",
    "\n",
    "        return last\n",
    "\n",
    "class GalaForce(pt.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.potential = GalaPotential(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (r, v) = x\n",
    "        r = pt.as_tensor(r)\n",
    "        v = pt.as_tensor(v)\n",
    "        r.requires_grad = True\n",
    "\n",
    "        potential = self.potential((r, v)).sum()\n",
    "        result = pt.autograd.grad(potential, r, create_graph=True)\n",
    "        assert len(result) == 1\n",
    "        return result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowws\n",
    "from flowws import Argument as Arg\n",
    "\n",
    "@flowws.add_stage_arguments\n",
    "class MoleculeForceRegression(flowws.Stage):\n",
    "    \"\"\"Build a geometric attention network for the molecular force regression task.\n",
    "\n",
    "    This module specifies the architecture of a network to calculate\n",
    "    atomic forces given the coordinates and types of atoms in a\n",
    "    molecule. Conservative forces are computed by calculating the\n",
    "    gradient of a scalar.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ARGS = [\n",
    "        Arg('rank', None, int, 2,\n",
    "            help='Degree of correlations (n-vectors) to consider'),\n",
    "        Arg('n_dim', '-n', int, 32,\n",
    "            help='Working dimensionality of point representations'),\n",
    "        Arg('dilation', None, float, 2,\n",
    "            help='Working dimension dilation factor for MLP components'),\n",
    "        Arg('merge_fun', '-m', str, 'concat',\n",
    "            help='Method to merge point representations'),\n",
    "        Arg('join_fun', '-j', str, 'concat',\n",
    "            help='Method to join invariant and point representations'),\n",
    "        Arg('n_blocks', '-b', int, 2,\n",
    "            help='Number of deep blocks to use'),\n",
    "        Arg('block_nonlinearity', None, bool, True,\n",
    "            help='If True, add a nonlinearity to the end of each block'),\n",
    "        Arg('residual', '-r', bool, True,\n",
    "            help='If True, use residual connections within blocks'),\n",
    "        Arg('invariant_mode', None, str, 'single',\n",
    "           help='Attention invariant_mode to use'),\n",
    "        Arg('invar_value_normalization', None, str, 'momentum',\n",
    "           help='Normalization applied to rotation-invariant attributes'),\n",
    "        Arg('value_normalization', None, str, 'momentum',\n",
    "           help='Normalization applied to value function hidden layer'),\n",
    "        Arg('score_normalization', None, str, 'momentum',\n",
    "           help='Normalization applied to score function hidden layer'),\n",
    "        Arg('block_normalization', None, str, 'momentum',\n",
    "           help='Normalization applied to post-block values'),\n",
    "    ]\n",
    "\n",
    "    def run(self, scope, storage):\n",
    "        D_in = scope['num_types']\n",
    "\n",
    "        model = GalaForce(\n",
    "            D_in, self.arguments['n_dim'], self.arguments['n_blocks'],\n",
    "            self.arguments['dilation'], self.arguments['residual'],\n",
    "            self.arguments['block_nonlinearity'], self.arguments['merge_fun'],\n",
    "            self.arguments['join_fun'], self.arguments['invariant_mode'],\n",
    "            self.arguments['rank'], self.arguments['invar_value_normalization'],\n",
    "            self.arguments['value_normalization'], self.arguments['score_normalization'],\n",
    "            self.arguments['block_normalization'],\n",
    "        )\n",
    "\n",
    "        scope['model'] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowws\n",
    "from flowws import Argument as Arg\n",
    "import numpy as np\n",
    "\n",
    "def tensify(arrs):\n",
    "    if isinstance(arrs, (list, tuple)):\n",
    "        return tuple(tensify(arr) for arr in arrs)\n",
    "    return pt.as_tensor(arrs.astype(np.float32))\n",
    "\n",
    "OPTIMIZER_NAME_DICT = {\n",
    "    name.lower(): obj for (name, obj) in vars(pt.optim).items()\n",
    "    if type(obj) == type and issubclass(obj, pt.optim.Optimizer) and name != 'Optimizer'\n",
    "}\n",
    "\n",
    "@flowws.add_stage_arguments\n",
    "class Train(flowws.Stage):\n",
    "    \"\"\"Train a model using pytorch.\n",
    "    \"\"\"\n",
    "\n",
    "    ARGS = [\n",
    "        Arg('batch_size', '-b', int, 32,\n",
    "           help='Batch size to use for training'),\n",
    "        Arg('optimizer', '-o', str, 'adam',\n",
    "           help='Optimizer to use'),\n",
    "        Arg('epochs', '-e', int, 16,\n",
    "           help='Number of epochs to run'),\n",
    "        Arg('seed', '-s', int, 13,\n",
    "           help='Random seed to use for training'),\n",
    "        Arg('accumulate_gradients', '-g', int, 1,\n",
    "           help='Accumulate gradients over the given number of batches'),\n",
    "        Arg('verbose', '-v', bool, False,\n",
    "           help='Print more info during training'),\n",
    "        Arg('gpu', None, bool, True,\n",
    "           help='Try to use the GPU'),\n",
    "        Arg('optimizer_kwargs', None, [(str, eval)],\n",
    "           help='Keyword arguments to be passed to optimizer initialization')\n",
    "    ]\n",
    "\n",
    "    def run(self, scope, storage):\n",
    "        train_data = list(scope['x_train']) + [scope['y_train']]\n",
    "        val_data = list(scope['validation_data'][0]) + [scope['validation_data'][1]]\n",
    "\n",
    "        batch_size = self.arguments['batch_size']\n",
    "        train_data = pt.utils.data.DataLoader(\n",
    "            pt.utils.data.TensorDataset(*tensify(train_data)), batch_size=batch_size, pin_memory=True)\n",
    "        val_data = pt.utils.data.DataLoader(\n",
    "            pt.utils.data.TensorDataset(*tensify(val_data)), batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "        if self.arguments['gpu'] and pt.cuda.is_available():\n",
    "            device = pt.device('cuda')\n",
    "        else:\n",
    "            device = pt.device('cpu')\n",
    "\n",
    "        model = scope['model'].to(device)\n",
    "\n",
    "        Optimizer = OPTIMIZER_NAME_DICT[self.arguments['optimizer']]\n",
    "        kwargs = dict(self.arguments.get('optimizer_kwargs', []))\n",
    "        opt = Optimizer(model.parameters(), **kwargs)\n",
    "\n",
    "        def loop(dataset, train=True):\n",
    "            if train:\n",
    "                model.train()\n",
    "                opt.zero_grad()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            stats = []\n",
    "            for i, d in enumerate(dataset):\n",
    "                (r, v, y) = [a.to(device) for a in d]\n",
    "                x = (r, v)\n",
    "                pred = model(x)\n",
    "                loss = pt.nn.MSELoss()(pred, y)\n",
    "\n",
    "                if train:\n",
    "                    (loss/self.arguments['accumulate_gradients']).backward()\n",
    "#                     loss.backward()\n",
    "\n",
    "                if train and (i + 1)%self.arguments['accumulate_gradients'] == 0:\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "\n",
    "                scaled_mae = pt.mean(pt.abs(pred - y))*scope['y_scale']\n",
    "                stats.append((loss.detach().item(), scaled_mae.detach().item()))\n",
    "\n",
    "                if self.arguments['verbose']:\n",
    "                    print('batch', i, stats[-1])\n",
    "            return stats\n",
    "\n",
    "        for epoch in range(self.arguments['epochs']):\n",
    "            stats = loop(train_data)\n",
    "            print('epoch', epoch, 'train stats', *np.mean(stats, axis=0))\n",
    "            stats = loop(val_data, False)\n",
    "            print('epoch', epoch, 'val stats', *np.mean(stats, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = flowws.Workflow(\n",
    "    [\n",
    "        RMD17(\n",
    "            seed=13,\n",
    "            cache_dir=\"/tmp\",\n",
    "            n_train=1000,\n",
    "            n_val=1000,\n",
    "            y_scale_reduction=4,\n",
    "            x_scale_reduction=-1,\n",
    "            molecules=[\n",
    "                \"malonaldehyde\",\n",
    "            ],\n",
    "            no_keras=True,\n",
    "        ),\n",
    "        MoleculeForceRegression(\n",
    "            n_dim=32,\n",
    "            n_blocks=3,\n",
    "            invariant_mode='single',\n",
    "            merge_fun='mean',\n",
    "            join_fun='mean',\n",
    "            invar_value_normalization='momentum',\n",
    "            value_normalization='layer',\n",
    "            score_normalization='layer',\n",
    "            block_normalization='layer',\n",
    "            residual=True,\n",
    "        ),\n",
    "        Train(\n",
    "            epochs=40,\n",
    "            batch_size=4,\n",
    "            accumulate_gradients=32,\n",
    "        ),\n",
    "    ],\n",
    "    storage=flowws.DirectoryStorage(\"/tmp\"),\n",
    ")\n",
    "\n",
    "scope = w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Molecular force regression using multivectors in pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
