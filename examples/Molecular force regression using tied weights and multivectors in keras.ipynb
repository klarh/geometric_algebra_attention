{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klarh/geometric_algebra_attention/blob/master/examples/Molecular%20force%20regression%20using%20tied%20weights%20and%20multivectors%20in%20keras.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Colab-specific setup that will be ignored elsewhere\n",
    "if [ ! -z \"$COLAB_GPU\" ]; then\n",
    "    pip install flowws-keras-geometry keras-gtar\n",
    "    pip install --force-reinstall git+https://github.com/klarh/flowws-keras-experimental\n",
    "    pip install git+https://github.com/klarh/geometric_algebra_attention\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowws\n",
    "from flowws_keras_geometry.data import RMD17\n",
    "from flowws_keras_experimental import InitializeTF, Train, Save\n",
    "import geometric_algebra_attention.keras as gala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowws_keras_geometry.models.internal import GradientLayer, \\\n",
    "    NeighborhoodReduction, \\\n",
    "    PairwiseValueNormalization, PairwiseVectorDifference, \\\n",
    "    PairwiseVectorDifferenceSum\n",
    "\n",
    "from geometric_algebra_attention import keras as gala\n",
    "from geometric_algebra_attention.tensorflow.geometric_algebra import custom_norm\n",
    "\n",
    "import flowws\n",
    "from flowws import Argument as Arg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "LAMBDA_ACTIVATIONS = {\n",
    "    'log1pswish': lambda x: tf.math.log1p(tf.nn.swish(x)),\n",
    "    'sin': tf.sin,\n",
    "    'leakyswish': lambda x: tf.nn.swish(x) - 1e-2*tf.nn.swish(-x)\n",
    "}\n",
    "\n",
    "NORMALIZATION_LAYERS = {\n",
    "    'batch': lambda _: keras.layers.BatchNormalization(),\n",
    "    'layer': lambda _: keras.layers.LayerNormalization(),\n",
    "    'momentum': lambda _: gala.MomentumNormalization(.9),\n",
    "    'momentum_layer': lambda _: gala.MomentumLayerNormalization(.9),\n",
    "}\n",
    "\n",
    "NORMALIZATION_LAYER_DOC = ' (any of {})'.format(','.join(NORMALIZATION_LAYERS))\n",
    "\n",
    "class NoisifyMultivector(keras.layers.Layer):\n",
    "    def __init__(self, scale=1e-7, **kwargs):\n",
    "        self.scale = scale\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        sh = tf.shape(x)\n",
    "        return x + tf.random.normal(sh[1:], stddev=self.scale)\n",
    "\n",
    "    def get_config(self):\n",
    "        result = super().get_config()\n",
    "        result['scale'] = self.scale\n",
    "        return result\n",
    "\n",
    "@flowws.add_stage_arguments\n",
    "class MoleculeForceRegression(flowws.Stage):\n",
    "    \"\"\"Build a geometric attention network for the molecular force regression task.\n",
    "\n",
    "    This module specifies the architecture of a network to calculate\n",
    "    atomic forces given the coordinates and types of atoms in a\n",
    "    molecule. Conservative forces are computed by calculating the\n",
    "    gradient of a scalar.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ARGS = [\n",
    "        Arg('rank', None, int, 2,\n",
    "            help='Degree of correlations (n-vectors) to consider'),\n",
    "        Arg('n_dim', '-n', int, 32,\n",
    "            help='Working dimensionality of point representations'),\n",
    "        Arg('dilation', None, float, 2,\n",
    "            help='Working dimension dilation factor for MLP components'),\n",
    "        Arg('merge_fun', '-m', str, 'concat',\n",
    "            help='Method to merge point representations'),\n",
    "        Arg('join_fun', '-j', str, 'concat',\n",
    "            help='Method to join invariant and point representations'),\n",
    "        Arg('dropout', '-d', float, 0,\n",
    "            help='Dropout rate to use, if any'),\n",
    "        Arg('mlp_layers', None, int, 1,\n",
    "            help='Number of hidden layers for score/value MLPs'),\n",
    "        Arg('n_blocks', '-b', int, 2,\n",
    "            help='Number of deep blocks to use'),\n",
    "        Arg('block_nonlinearity', None, bool, True,\n",
    "            help='If True, add a nonlinearity to the end of each block'),\n",
    "        Arg('residual', '-r', bool, True,\n",
    "            help='If True, use residual connections within blocks'),\n",
    "        Arg('activation', '-a', str, 'swish',\n",
    "            help='Activation function to use inside the network'),\n",
    "        Arg('final_activation', None, str, 'swish',\n",
    "            help='Final activation function to use within the network'),\n",
    "        Arg('score_normalization', None, [str], [],\n",
    "            help=('Normalizations to apply to score (attention) function' +\n",
    "                  NORMALIZATION_LAYER_DOC)),\n",
    "        Arg('value_normalization', None, [str], [],\n",
    "            help=('Normalizations to apply to value function' +\n",
    "                  NORMALIZATION_LAYER_DOC)),\n",
    "        Arg('block_normalization', None, [str], [],\n",
    "            help=('Normalizations to apply to the output of each attention block' +\n",
    "                  NORMALIZATION_LAYER_DOC)),\n",
    "        Arg('invariant_value_normalization', None, [str], [],\n",
    "            help=('Normalizations to apply to value function, before MLP layers' +\n",
    "                  NORMALIZATION_LAYER_DOC)),\n",
    "        Arg('equivariant_value_normalization', None, [str], [],\n",
    "            help=('Normalizations to apply to equivariant results' +\n",
    "                  NORMALIZATION_LAYER_DOC)),\n",
    "        Arg('invariant_mode', None, str, 'single',\n",
    "           help='Attention invariant_mode to use'),\n",
    "        Arg('covariant_mode', None, str, 'single',\n",
    "           help='Multivector2MultivectorAttention covariant_mode to use'),\n",
    "        Arg('include_normalized_products', None, bool, False,\n",
    "           help='Also include normalized geometric product terms'),\n",
    "        Arg('normalize_equivariant_values', None, bool, False,\n",
    "           help='If True, divide equivariant values by magnitude of inputs after each attention step'),\n",
    "    ]\n",
    "\n",
    "    def run(self, scope, storage):\n",
    "        rank = self.arguments['rank']\n",
    "\n",
    "        if self.arguments['activation'] in LAMBDA_ACTIVATIONS:\n",
    "            activation_layer = lambda: keras.layers.Lambda(\n",
    "                LAMBDA_ACTIVATIONS[self.arguments['activation']])\n",
    "        else:\n",
    "            activation_layer = lambda: keras.layers.Activation(\n",
    "                self.arguments['activation'])\n",
    "\n",
    "        if self.arguments['final_activation'] in LAMBDA_ACTIVATIONS:\n",
    "            final_activation_layer = lambda: keras.layers.Lambda(\n",
    "                LAMBDA_ACTIVATIONS[self.arguments['final_activation']])\n",
    "        else:\n",
    "            final_activation_layer = lambda: keras.layers.Activation(\n",
    "                self.arguments['final_activation'])\n",
    "\n",
    "        n_dim = self.arguments['n_dim']\n",
    "        dilation_dim = int(np.round(n_dim*self.arguments['dilation']))\n",
    "        \n",
    "        def make_layer_inputs(x, v):\n",
    "            nonnorm = (x, v)\n",
    "            if self.arguments['normalize_equivariant_values']:\n",
    "                xnorm = keras.layers.LayerNormalization()(x)\n",
    "                norm = (xnorm, v)\n",
    "                return [nonnorm] + (rank - 1) * [norm]\n",
    "            else:\n",
    "                return rank * [nonnorm]\n",
    "\n",
    "        def make_scorefun():\n",
    "            layers = []\n",
    "\n",
    "            for _ in range(self.arguments['mlp_layers']):\n",
    "                layers.append(keras.layers.Dense(dilation_dim))\n",
    "\n",
    "                for name in self.arguments['score_normalization']:\n",
    "                    layers.append(NORMALIZATION_LAYERS[name](rank))\n",
    "\n",
    "                layers.append(activation_layer())\n",
    "\n",
    "                if self.arguments.get('dropout', 0):\n",
    "                    layers.append(keras.layers.Dropout(self.arguments['dropout']))\n",
    "\n",
    "            layers.append(keras.layers.Dense(1))\n",
    "            return keras.models.Sequential(layers)\n",
    "\n",
    "        def make_valuefun(n_dim=n_dim, in_network=True, activation=None):\n",
    "            layers = []\n",
    "\n",
    "            if in_network:\n",
    "                for name in self.arguments['invariant_value_normalization']:\n",
    "                    layers.append(NORMALIZATION_LAYERS[name](rank))\n",
    "\n",
    "            for _ in range(self.arguments['mlp_layers']):\n",
    "                layers.append(keras.layers.Dense(dilation_dim))\n",
    "\n",
    "                for name in self.arguments['value_normalization']:\n",
    "                    layers.append(NORMALIZATION_LAYERS[name](rank))\n",
    "\n",
    "                layers.append(activation_layer())\n",
    "\n",
    "                if self.arguments.get('dropout', 0):\n",
    "                    layers.append(keras.layers.Dropout(self.arguments['dropout']))\n",
    "\n",
    "            if activation in LAMBDA_ACTIVATIONS:\n",
    "                layers.append(keras.layers.Dense(n_dim))\n",
    "                layers.append(keras.layers.Lambda(LAMBDA_ACTIVATIONS[activation]))\n",
    "            else:\n",
    "                layers.append(keras.layers.Dense(n_dim, activation=activation))\n",
    "            return keras.models.Sequential(layers)\n",
    "\n",
    "        def make_block(x_last, last):\n",
    "            residual_in = last\n",
    "            residual_in_x = x_last\n",
    "\n",
    "            (x_last, last) = gala.TiedMultivectorAttention(\n",
    "                make_scorefun(), make_valuefun(), make_valuefun(1), False, rank=rank,\n",
    "                join_fun=self.arguments['join_fun'],\n",
    "                merge_fun=self.arguments['merge_fun'],\n",
    "                invariant_mode=self.arguments['invariant_mode'],\n",
    "                covariant_mode=self.arguments['covariant_mode'],\n",
    "                include_normalized_products=self.arguments['include_normalized_products'],\n",
    "            )(make_layer_inputs(x_last, last))\n",
    "\n",
    "            if self.arguments['block_nonlinearity']:\n",
    "                last = make_valuefun(in_network=False)(last)\n",
    "\n",
    "            if self.arguments['residual']:\n",
    "                last = last + residual_in\n",
    "                x_last = x_last + residual_in_x\n",
    "\n",
    "            for name in self.arguments['equivariant_value_normalization']:\n",
    "                x_last = NORMALIZATION_LAYERS[name](rank)(x_last)\n",
    "\n",
    "            for name in self.arguments.get('block_normalization', []):\n",
    "                last = NORMALIZATION_LAYERS[name](rank)(last)\n",
    "\n",
    "            return x_last, last\n",
    "\n",
    "        x_in = keras.layers.Input((scope['neighborhood_size'], 3))\n",
    "        v_in = keras.layers.Input((scope['neighborhood_size'], scope['num_types']))\n",
    "\n",
    "        delta_x = PairwiseVectorDifference()(x_in)\n",
    "        delta_v = PairwiseVectorDifferenceSum()(v_in)\n",
    "\n",
    "        delta_x = NoisifyMultivector(1e-7)(delta_x)\n",
    "\n",
    "        x_last = gala.Vector2Multivector()(delta_x)\n",
    "\n",
    "        last = keras.layers.Dense(n_dim)(delta_v)\n",
    "        for _ in range(self.arguments['n_blocks']):\n",
    "            x_last, last = make_block(x_last, last)\n",
    "\n",
    "        (last, ivs, att) = gala.MultivectorAttention(\n",
    "            make_scorefun(), make_valuefun(), True, name='final_attention',\n",
    "            rank=rank,\n",
    "            join_fun=self.arguments['join_fun'],\n",
    "            merge_fun=self.arguments['merge_fun'],\n",
    "            invariant_mode=self.arguments['invariant_mode'],\n",
    "            include_normalized_products=self.arguments['include_normalized_products'],\n",
    "        )(\n",
    "            make_layer_inputs(x_last, last), return_invariants=True, return_attention=True)\n",
    "\n",
    "        last = keras.layers.Dense(dilation_dim, name='final_mlp')(last)\n",
    "        last = final_activation_layer()(last)\n",
    "        last = keras.layers.Dense(1, name='energy_projection', use_bias=False)(last)\n",
    "        last = NeighborhoodReduction()(last)\n",
    "        last = GradientLayer()((last, x_in))\n",
    "\n",
    "        scope['input_symbol'] = [x_in, v_in]\n",
    "        scope['output'] = last\n",
    "        scope['loss'] = 'mse'\n",
    "        scope['attention_model'] = keras.models.Model([x_in, v_in], att)\n",
    "        scope['invariant_model'] = keras.models.Model([x_in, v_in], ivs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w = flowws.Workflow(\n",
    "    [\n",
    "        InitializeTF(),\n",
    "        RMD17(\n",
    "            seed=13,\n",
    "            cache_dir=\"/tmp\",\n",
    "            n_train=1000,\n",
    "            n_val=1000,\n",
    "            y_scale_reduction=16,\n",
    "            x_scale_reduction=16,\n",
    "            molecules=[\n",
    "                \"benzene\",\n",
    "            ],\n",
    "        ),\n",
    "        MoleculeForceRegression(\n",
    "            n_dim=32,\n",
    "            n_blocks=3,\n",
    "            invariant_mode='single',\n",
    "            covariant_mode='single',\n",
    "            activation='swish',\n",
    "            merge_fun='mean',\n",
    "            join_fun='mean',\n",
    "            block_normalization=['layer'],\n",
    "            score_normalization=['layer'],\n",
    "            value_normalization=['layer'],\n",
    "            invariant_value_normalization=['momentum'],\n",
    "            equivariant_value_normalization=['momentum_layer'],\n",
    "            residual=True,\n",
    "            include_normalized_products=True,\n",
    "            normalize_equivariant_values=True,\n",
    "            mlp_layers=2,\n",
    "            dropout=.1,\n",
    "        ),\n",
    "        Train(\n",
    "            epochs=40,\n",
    "            reduce_lr=25,\n",
    "            early_stopping=70,\n",
    "            batch_size=4,\n",
    "            validation_split=0,\n",
    "            reduce_lr_factor=0.8,\n",
    "            early_stopping_best=1,\n",
    "            accumulate_gradients=32,\n",
    "            catch_keyboard_interrupt=True,\n",
    "        ),\n",
    "    ],\n",
    "    storage=flowws.DirectoryStorage(\"/tmp\"),\n",
    ")\n",
    "\n",
    "scope = w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "\n",
    "y = np.concatenate([series[1]['mean_absolute_error'] for series in scope['log_quantities']])\n",
    "pp.plot(y, label='train set')\n",
    "y = np.concatenate([series[1]['val_mean_absolute_error'] for series in scope['log_quantities']])\n",
    "pp.plot(y, label='val set')\n",
    "pp.gca().set_yscale('log')\n",
    "pp.xlabel('Epoch'); pp.ylabel('MAE')\n",
    "pp.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Molecular force regression using multivectors in keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
